# agent/llm_handler.py
import logging
from openai import OpenAI, APIConnectionError, RateLimitError
import config

logger = logging.getLogger(__name__)

class LLMHandler:
    def __init__(self, client: OpenAI):
        self.client = client
        self.system_prompt = config.DEFAULT_SYSTEM_PROMPT

    def get_response(self, text_to_refute: str):
        if not self.client:
            logger.error("LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼")
            return

        logger.info("[ğŸ¤– AIæ ç²¾ æ­£åœ¨æ€è€ƒ...]")
        
        try:
            completion = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {'role': 'system', 'content': self.system_prompt},
                    {'role': 'user', 'content': text_to_refute}
                ],
                stream=True,
            )

            logger.info("[ğŸ¤– AIæ ç²¾ ç”Ÿæˆä¸­...]")
            response_text = ""
            for chunk in completion:
                content = chunk.choices[0].delta.content
                if content:
                    response_text += content
                    # å¯¹äºæµå¼è¾“å‡ºï¼Œæˆ‘ä»¬ä»ç„¶ä½¿ç”¨printæ¥ä¿æŒå®æ—¶æ˜¾ç¤ºæ•ˆæœ
                    print(content, end="", flush=True)
            print()  # æ¢è¡Œ
            logger.info(f"AIå›å¤å®Œæˆ: {response_text}")

        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ•è·æ›´å…·ä½“çš„å¼‚å¸¸ ---
        except APIConnectionError as e:
            logger.error(f"LLMç½‘ç»œè¿æ¥å¤±è´¥: {e.__cause__}")
        except RateLimitError as e:
            logger.warning("LLMè¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")
        except Exception as e:
            logger.error(f"è°ƒç”¨å¤§æ¨¡å‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}", exc_info=True)
