# agent/llm_handler.py
from openai import OpenAI, APIConnectionError, RateLimitError
import config

class LLMHandler:
    def __init__(self, client: OpenAI):
        self.client = client
        self.system_prompt = config.DEFAULT_SYSTEM_PROMPT

    def get_response(self, text_to_refute: str):
        if not self.client:
            print("\n[é”™è¯¯] LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼")
            return

        print("\n[ğŸ¤– AIæ ç²¾ æ­£åœ¨æ€è€ƒ...]")
        
        try:
            completion = self.client.chat.completions.create(
                model=config.LLM_MODEL,
                messages=[
                    {'role': 'system', 'content': self.system_prompt},
                    {'role': 'user', 'content': text_to_refute}
                ],
                stream=True,
            )
            
            print("ğŸ¤– AIæ ç²¾: ", end="")
            for chunk in completion:
                content = chunk.choices[0].delta.content
                if content:
                    print(content, end="", flush=True)
            print("\n")

        # --- æ ¸å¿ƒæ”¹åŠ¨ï¼šæ•è·æ›´å…·ä½“çš„å¼‚å¸¸ ---
        except APIConnectionError as e:
            print(f"\n[LLMé”™è¯¯] ç½‘ç»œè¿æ¥å¤±è´¥: {e.__cause__}")
        except RateLimitError as e:
            print(f"\n[LLMé”™è¯¯] è¯·æ±‚é¢‘ç‡è¿‡é«˜ï¼Œè¯·ç¨åå†è¯•ã€‚")
        except Exception as e:
            print(f"\n[LLMé”™è¯¯] è°ƒç”¨å¤§æ¨¡å‹æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")