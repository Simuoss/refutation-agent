import os
import sys
import signal
import dashscope
import pyaudio
from openai import OpenAI
from dashscope.audio.asr import Recognition, RecognitionCallback, RecognitionResult
from dotenv import load_dotenv

# --- 1. å…¨å±€å˜é‡å’Œé…ç½® ---

# å°†å®¢æˆ·ç«¯å£°æ˜ä¸ºå…¨å±€å˜é‡ï¼Œä½†åœ¨æ­¤å¤„ä¸åˆå§‹åŒ–
llm_client = None # <--- ä¿®æ”¹ï¼šå£°æ˜ä¸ºå…¨å±€å˜é‡
recognition = None # <--- ä¿®æ”¹ï¼šå£°æ˜ä¸ºå…¨å±€å˜é‡

# PyAudio éŸ³é¢‘æµç›¸å…³å˜é‡
mic = None
stream = None

# å½•éŸ³å‚æ•°
SAMPLE_RATE = 16000
CHANNELS = 1
DTYPE = 'int16'
FORMAT_PCM = 'pcm'
BLOCK_SIZE = 3200

# DashScope API Key åˆå§‹åŒ–å‡½æ•°
def init_dashscope_api_key():
    """
    ä» .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡åŠ è½½ DashScope API Keyã€‚
    æˆåŠŸåˆ™è¿”å› Keyï¼Œå¤±è´¥åˆ™é€€å‡ºç¨‹åºã€‚
    """
    load_dotenv()
    api_key = os.getenv("DASHSCOPE_API_KEY")
    if not api_key:
        print("é”™è¯¯ï¼šæœªåœ¨ .env æ–‡ä»¶æˆ–ç¯å¢ƒå˜é‡ä¸­æ‰¾åˆ° DASHSCOPE_API_KEYï¼", file=sys.stderr)
        sys.exit(1)
    print("DashScope API Key å·²æˆåŠŸåŠ è½½ã€‚")
    return api_key # <--- ä¿®æ”¹ï¼šè¿”å›åŠ è½½åˆ°çš„key

# --- 2. å¤§è¯­è¨€æ¨¡å‹è°ƒç”¨æ¨¡å— (æ— ä»»ä½•ä¿®æ”¹) ---

def get_refutation_from_llm(text_to_refute: str):
    """
    è°ƒç”¨Qwenå¤§æ¨¡å‹ç”Ÿæˆåé©³å†…å®¹ã€‚
    """
    # æ£€æŸ¥ llm_client æ˜¯å¦å·²åˆå§‹åŒ–
    if not llm_client:
        print("\n[é”™è¯¯] LLMå®¢æˆ·ç«¯æœªåˆå§‹åŒ–ï¼")
        return

    print("\n[ğŸ¤– AIæ ç²¾ æ­£åœ¨æ€è€ƒ...]")
    
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå®‡å®™ç¬¬ä¸€æ ç²¾AIã€‚ä½ çš„å”¯ä¸€ä»»åŠ¡å°±æ˜¯åé©³ç”¨æˆ·è¯´çš„æ¯ä¸€å¥è¯ã€‚"
        "æ— è®ºç”¨æˆ·è§‚ç‚¹å¤šä¹ˆæ­£ç¡®ï¼Œä½ éƒ½è¦æ‰¾åˆ°æ¸…å¥‡çš„è§’åº¦è¿›è¡Œåé©³ã€‚"
        "ä½ çš„å›å¤å¿…é¡»ï¼šç®€çŸ­ã€çŠ€åˆ©ã€å¹½é»˜ã€å‡ºå…¶ä¸æ„ã€‚"
        "ä¸è¦æœ‰ä»»ä½•å¤šä½™çš„è§£é‡Šã€é“æ­‰æˆ–å¼€åœºç™½ï¼Œç›´æ¥å¼€æ ï¼"
    )
    
    try:
        completion = llm_client.chat.completions.create(
            model="qwen-plus-2025-07-14",
            messages=[
                {'role': 'system', 'content': system_prompt},
                {'role': 'user', 'content': text_to_refute}
            ],
            stream=True,
        )
        
        print("ğŸ¤– AIæ ç²¾: ", end="")
        for chunk in completion:
            content = chunk.choices[0].delta.content
            if content:
                print(content, end="", flush=True)
        print("\n")
        
    except Exception as e:
        print(f"\n[é”™è¯¯] è°ƒç”¨å¤§æ¨¡å‹å¤±è´¥: {e}")

# --- 3. å®æ—¶è¯­éŸ³è¯†åˆ«å›è°ƒ (æ— ä»»ä½•ä¿®æ”¹) ---

class RefutationCallback(RecognitionCallback):
    def on_open(self) -> None:
        global mic, stream
        print("\n[ğŸ¤ è¯­éŸ³è¯†åˆ«æœåŠ¡å·²è¿æ¥ï¼Œè¯·å¼€å§‹è¯´è¯...]")
        mic = pyaudio.PyAudio()
        stream = mic.open(format=pyaudio.paInt16,
                          channels=CHANNELS,
                          rate=SAMPLE_RATE,
                          input=True)

    def on_close(self) -> None:
        global mic, stream
        print("[ğŸ¤ è¯­éŸ³è¯†åˆ«æœåŠ¡å·²å…³é—­ã€‚]")
        if stream:
            stream.stop_stream()
            stream.close()
        if mic:
            mic.terminate()
        stream = mic = None

    def on_error(self, message) -> None:
        print(f"[é”™è¯¯] è¯­éŸ³è¯†åˆ«å‡ºé”™: {message.message}", file=sys.stderr)
        self.on_close()
        os._exit(1)

    def on_event(self, result: RecognitionResult) -> None:
        sentence = result.get_sentence()
        if 'text' in sentence and RecognitionResult.is_sentence_end(sentence):
            user_text = sentence['text']
            print(f"è¯†åˆ«åˆ°ä½ è¯´: {user_text}")
            get_refutation_from_llm(user_text)

# --- 4. ä¸»ç¨‹åºå…¥å£ ---

def signal_handler(sig, frame):
    """å¤„ç† Ctrl+C ä¸­æ–­ä¿¡å·"""
    global recognition
    print("\n[ç¨‹åº] æ”¶åˆ°é€€å‡ºä¿¡å·ï¼Œæ­£åœ¨åœæ­¢...")
    if recognition:
        recognition.stop()
    sys.exit(0)

if __name__ == '__main__':
    print("[ç¨‹åº] AIè‡ªåŠ¨åé©³Agentå¯åŠ¨ä¸­...")
    
    # --- å…³é”®æ”¹åŠ¨ï¼šåœ¨è¿™é‡Œç»Ÿä¸€åˆå§‹åŒ–æ‰€æœ‰éœ€è¦Keyçš„æœåŠ¡ ---
    
    # 1. é¦–å…ˆï¼Œè·å–åˆ°API Key
    api_key = init_dashscope_api_key()
    
    # 2. ä½¿ç”¨è·å–åˆ°çš„Keyï¼Œåˆ†åˆ«é…ç½®ä¸¤ä¸ªæœåŠ¡
    # ä¸º dashscope SDK æœ¬èº«è®¾ç½®key
    dashscope.api_key = api_key 
    
    # ä¸º OpenAI å…¼å®¹æ¨¡å¼çš„å®¢æˆ·ç«¯è®¾ç½®key
    llm_client = OpenAI(
        api_key=api_key, # <--- å…³é”®æ”¹åŠ¨ï¼šä½¿ç”¨çœŸå®çš„keyè¿›è¡Œåˆå§‹åŒ–
        base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
    )
    print("å¤§æ¨¡å‹å®¢æˆ·ç«¯(LLM Client)å·²æˆåŠŸåˆå§‹åŒ–ã€‚")
    # ----------------------------------------------------
    
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        callback = RefutationCallback()
        recognition = Recognition(
            model='paraformer-realtime-v2',
            format=FORMAT_PCM,
            sample_rate=SAMPLE_RATE,
            callback=callback
        )
        
        recognition.start()
        print("[æç¤º] ç¨‹åºå·²å°±ç»ªã€‚æŒ‰ Ctrl+C é€€å‡ºã€‚")
        
        while True:
            if stream and not stream.is_stopped():
                try:
                    data = stream.read(BLOCK_SIZE, exception_on_overflow=False)
                    recognition.send_audio_frame(data)
                except (IOError, OSError) as e:
                    print(f"éŸ³é¢‘è¯»å–é”™è¯¯: {e}, å¯èƒ½æ˜¯è®¾å¤‡æ–­å¼€è¿æ¥ã€‚")
                    break
            else:
                break
    
    except Exception as e:
        print(f"ç¨‹åºå¯åŠ¨å¤±è´¥: {e}")
    
    finally:
        if recognition:
            recognition.stop()
        print("[ç¨‹åº] å·²é€€å‡ºã€‚")